{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z64okKETGRTs"
   },
   "source": [
    "# Neural network from scratch\n",
    "\n",
    "We will implement a basic neural network using only NumPy, a Python library providing support for arrays (vectors and matrices) and operations on those arrays.\n",
    "\n",
    "The neural network code is divided up into the following steps.\n",
    "* **Network architecture**, where we specify the number of layers and units in each layer.\n",
    "* **Weight initialization**, where we initialize the weights to small random values.\n",
    "* **Activation functions**, where we define the activation functions.\n",
    "* **Forward propagation**, where we specify all the computations associated with a forward pass throught the network.\n",
    "* **Error/cost function**, where we specify the error/cost function that will be minimized.\n",
    "* **Performance metric**, where we specify how to measure the networks performance.\n",
    "* **Back propagation**, where we specify all the computations necessary to update the weights.\n",
    "* **Training function**, a function that puts together all of the above componets and trains the network.\n",
    "\n",
    "Beyond the above parts, there is additional code for plotting, training and testing the network.\n",
    "\n",
    "\n",
    "## Required libraries\n",
    "\n",
    "Make sure you have the following libraries installed.\n",
    "\n",
    "* NumPy\n",
    "* Scikit-learn (sklearn)\n",
    "* Matplotlib\n",
    "* Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OMmSzSsuGRTv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPAKD-VxGRUA"
   },
   "source": [
    "## Network architecture\n",
    "\n",
    "\n",
    "\n",
    "![Network architecture](https://drive.google.com/uc?id=1-MSS49rORx9oMw4uBvbckHO1vx1fDV8i)\n",
    "\n",
    "<b>Figure 1.</b> Example of a dense neural network architecture\n",
    "\n",
    "The code below specifies an architecture that corresponds to Fig 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "P2ZBzdy7GRUH"
   },
   "outputs": [],
   "source": [
    "# 5-layer neural network\n",
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgcSr9mdGRUT"
   },
   "source": [
    "### Question 1\n",
    "  a) How many weights are there in the first weight layer?\n",
    "\n",
    "  b) How many weights are there in the whole network?\n",
    "\n",
    "## Initialize the weights in each layer\n",
    "\n",
    "The weights are initialized with small random numbers. ```np.random.randn()``` returns samples for the standard normal distribution (approximately from -3 to 3). The weights are stored in a ```dict``` called *weights*, where keys are named, for example, ```W1``` (weight vector for layer 1) and ```b1``` (bias vector for layer 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jFgBNbedGRUX"
   },
   "outputs": [],
   "source": [
    "def init_weights(nn_architecture, seed=1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # random seed initiation in order to get reproducible results\n",
    "    np.random.seed(seed)\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    # Initialize weight storage\n",
    "    weights = {}\n",
    "    \n",
    "    # iterate over network layers\n",
    "    for i, layer in enumerate(nn_architecture):\n",
    "        # layers are numbered from 1\n",
    "        layer_i = i + 1\n",
    "        \n",
    "        # extract the number of units the layer\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        # initiate the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        weights['W%d' % layer_i] = np.random.randn(layer_output_size, layer_input_size) * 0.1\n",
    "        weights['b%d' % layer_i] = np.random.randn(layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsLJYiR7GRUh"
   },
   "source": [
    "### Question 2\n",
    "Inspect the code in the above cell. How many, if any, biases are there in the network?\n",
    "\n",
    "## Activation functions\n",
    "\n",
    "We will use only two activation functions: ```sigmoid()``` for the final output (since the network should do binary classification) and ```relu()``` for the hidden nodes. Figure 2 shows these two activation functions and their corresponding derivatives.\n",
    "\n",
    "![Activations](https://drive.google.com/uc?id=1C-8wXvals_eI--GdJxGfGphLKw1L26uE)\n",
    "\n",
    "<b>Figure 2.</b> Activation functions used in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ERuCL4zRGRUj"
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    for the final output\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    for the hidden nodes\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    sig = sigmoid(Z)\n",
    "    \n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvSBxbqhGRUu"
   },
   "source": [
    "## Forward propagation\n",
    "\n",
    "To make the code more readable, we divide it up into two functions: ```single_layer_fwdprop()``` and ```full_fwdprop()``` where the latter calls the former.\n",
    "\n",
    "### Single layer forward propagation\n",
    "\n",
    "It's a matrix multiplication (```np.dot()```) to which the biases are added, followed by passing the result through an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pblebNTIGRUw"
   },
   "outputs": [],
   "source": [
    "def single_layer_fwdprop(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # calculate the input value for the activation function\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "        \n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "        \n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    # return the calculated activation A and the intermediate Z values\n",
    "    return activation_func(Z_curr), Z_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q88PzALIGRU8"
   },
   "source": [
    "### Full forward propagation\n",
    "\n",
    "This iterates through the layers (from layer 1 to 5) calling ```single_layer_fwdprop()```. In addition to the predictions, ```A_curr```, a cache, ```cache```, of intermediate values for use by ```single_layer_backprop()``` are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "m_vxwgDBGRU-"
   },
   "outputs": [],
   "source": [
    "def full_fwdprop(X, weights, nn_architecture):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # creating a cache to store the information needed for a backward step\n",
    "    cache = {}\n",
    "    # X vector is the activation for layer 0â€Š\n",
    "    A_curr = X\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for i, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_i = i + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        # extraction of W for the current layer\n",
    "        W_curr = weights[\"W%d\" % layer_i]\n",
    "        # extraction of b for the current layer\n",
    "        b_curr = weights[\"b%d\" % layer_i]\n",
    "        # calculation of activation for the current layer\n",
    "        A_curr, Z_curr = single_layer_fwdprop(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        # saving calculated values in the cache\n",
    "        cache[\"A%d\" % i] = A_prev\n",
    "        cache[\"Z%d\" % layer_i] = Z_curr\n",
    "       \n",
    "    # return a prediction vector and a dictionary containing intermediate values\n",
    "    return A_curr, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61n__yamGRVH"
   },
   "source": [
    "## Error/cost function\n",
    "\n",
    "The error we use here is **binary crossentropy**, suitable of binary classification.\n",
    "\n",
    "\n",
    "\n",
    ">$error = -\\frac{1}{m} [Y \\cdot log(\\hat{Y}) + (1 - Y) \\cdot log(1 - \\hat{Y})]$\n",
    "\n",
    "![Binary crossentropy](./cost_function.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2iJIafPaGRVH"
   },
   "outputs": [],
   "source": [
    "def get_error(Y_hat, Y):\n",
    "    \"\"\"\n",
    "    Error/cost/loss\n",
    "    \n",
    "    Binary crossentropy\n",
    "    \"\"\"\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    # calculate the error/cost\n",
    "    error = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    \n",
    "    return np.squeeze(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzlFdeyqGRVP"
   },
   "source": [
    "## Performance metric -- average number of correct predictions\n",
    "\n",
    "### Probabilities to class\n",
    "The output of our network (```y_hat```) is a probability that the input belongs to one of two classes. We convert those probabilities to class predictions by thresholding at 0.5. That is, p > 0.5 becomes class 1 and p <= 0.5 class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6F7T3cKWGRVS"
   },
   "outputs": [],
   "source": [
    "# helper function that converts probability into class\n",
    "def prob2class(probs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    classes = np.copy(probs)\n",
    "    classes[probs > 0.5] = 1\n",
    "    classes[probs <= 0.5] = 0\n",
    "    \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaGUda19GRVa"
   },
   "source": [
    "### Accuracy\n",
    "The average number of correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hFEMAmLPGRVc"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(Y_hat, Y):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Y_hat_ = prob2class(Y_hat)\n",
    "    \n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkZvlwPQGRVl"
   },
   "source": [
    "## Back propagation\n",
    "\n",
    "As with forward propagation, for readability, we divide it up into two functions: ```single_layer_backprop()``` and ```full_backprop()``` where the latter calls the former.\n",
    "\n",
    "### Single layer backwards propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "yOy3ArL2GRVm"
   },
   "outputs": [],
   "source": [
    "def single_layer_backprop(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # select activation function\n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    # calculate the derivative of the activation function\n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    \n",
    "    # derivative of the matrix W\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    # derivative of the vector b\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    # derivative of the matrix A_prev\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vagaYj9OGRVu"
   },
   "source": [
    "### Full backwards propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "QvIRhbSUGRVw"
   },
   "outputs": [],
   "source": [
    "def full_backprop(Y_hat, Y, cache, weights, nn_architecture):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    gradients = {}\n",
    "    \n",
    "    # number of examples\n",
    "    m = Y.shape[1]\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # Initiate gradient descent\n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for prev_layer_i, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        # layers are numbered from 1\n",
    "        curr_layer_i = prev_layer_i + 1\n",
    "        # get of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = cache[\"A%d\" % prev_layer_i]\n",
    "        Z_curr = cache[\"Z%d\" % curr_layer_i]\n",
    "        \n",
    "        W_curr = weights[\"W%d\" % curr_layer_i]\n",
    "        b_curr = weights[\"b%d\" % curr_layer_i]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backprop(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        gradients[\"dW%d\" % curr_layer_i] = dW_curr\n",
    "        gradients[\"db%d\" % curr_layer_i] = db_curr\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugI_6q34GRV7"
   },
   "source": [
    "### Updating the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Lz3vV-s3GRV9"
   },
   "outputs": [],
   "source": [
    "def update(weights, gradients, nn_architecture, learning_rate):\n",
    "\n",
    "    # iteration over network layers\n",
    "    for layer_i, layer in enumerate(nn_architecture, 1):\n",
    "        weights[\"W%d\" % layer_i] -= learning_rate * gradients[\"dW%d\" % layer_i]        \n",
    "        weights[\"b%d\" % layer_i] -= learning_rate * gradients[\"db%d\" % layer_i]\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XukRamZGRWE"
   },
   "source": [
    "## Function to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Ipq0NEYGGRWG"
   },
   "outputs": [],
   "source": [
    "def train(X, Y, nn_architecture, n_epochs, learning_rate, verbose=False, callback=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # initiation of neural net weights\n",
    "    weights = init_weights(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    error_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    # performing calculations for subsequent iterations\n",
    "    for i in range(n_epochs):\n",
    "        # step forward\n",
    "        Y_hat, cache = full_fwdprop(X, weights, nn_architecture)\n",
    "        \n",
    "        # calculating metrics and saving them in history\n",
    "        error = get_error(Y_hat, Y)\n",
    "        error_history.append(error)\n",
    "        accuracy = get_accuracy(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        # step backward - calculating gradient\n",
    "        gradients = full_backprop(Y_hat, Y, cache, weights, nn_architecture)\n",
    "        # updating model state\n",
    "        weights = update(weights, gradients, nn_architecture, learning_rate)\n",
    "        \n",
    "        if (i % 100) == 0:\n",
    "            if verbose:\n",
    "                print(\"Epoch: {:05} - error: {:.5f} - accuracy: {:.5f}\".format(i+1, error, accuracy))\n",
    "\n",
    "        if (i % 100) == 0:\n",
    "            if callback is not None:\n",
    "                callback(i, weights, error)                \n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxPU3pUdGRWP"
   },
   "source": [
    "## Train and test the neural network\n",
    "\n",
    "### Generate a dataset using sklearn\n",
    "\n",
    "First we need some artificial data to test our network. For simplicity (and network architecture), we will have two classes (i.e. binary classification) and 2-dimensional inputs (i.e. each exemplar is made up of 2 values). We can use ```sklearn.datasets.make_moons()``` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "QG1fkfDEGRWR"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# number of examples in the data set\n",
    "n_examples = 1000\n",
    "# fraction of the data set that goes into the test set\n",
    "test_sz = 0.1\n",
    "\n",
    "X, y = make_moons(n_samples=n_examples, noise=0.2, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_sz, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmmranEzGRWY"
   },
   "source": [
    "### Plotting\n",
    "\n",
    "#### Helper function to make nice plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "TDTTXjteGRWa"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# the function making up the graph of a dataset\n",
    "def make_plot(X, y, plot_name, fname=None, XX=None, YY=None, preds=None, dark=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if (dark):\n",
    "        plt.style.use('dark_background')\n",
    "    else:\n",
    "        sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(16,12))\n",
    "    axes = plt.gca()\n",
    "    axes.set(xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "    plt.title(plot_name, fontsize=30)\n",
    "    plt.subplots_adjust(left=0.20)\n",
    "    plt.subplots_adjust(right=0.80)\n",
    "    \n",
    "    if XX is not None and YY is not None and preds is not None:\n",
    "        plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha=1, cmap=cm.Spectral)\n",
    "        plt.contour(XX, YY, preds.reshape(XX.shape), levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)\n",
    "        \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral, edgecolors='none')\n",
    "    \n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbrTUA7vGRWg"
   },
   "source": [
    "#### Visualize the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "494pkIl8GRWi"
   },
   "outputs": [],
   "source": [
    "make_plot(X, y, \"Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w2_YBmYGRWs"
   },
   "source": [
    "### Question 3\n",
    "Given this data set, could a linear decision boundary give a good separation of the two classes?\n",
    "\n",
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-57RLjsGRWt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# \n",
    "n_epochs = 10000\n",
    "learning_rate = 0.01\n",
    "weights = train(X_train.T, y_train.reshape((1, -1)), nn_architecture, n_epochs, learning_rate, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9X8_wO3GRW3"
   },
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "l05iPzykGRW4"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "Y_test_hat, _ = full_fwdprop(X_test.T, weights, nn_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7McDCDbGRW_"
   },
   "outputs": [],
   "source": [
    "Y_test_hat[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGQy60-aGRXG"
   },
   "source": [
    "### Check the performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdCLRTJvGRXJ"
   },
   "outputs": [],
   "source": [
    "# Accuracy achieved on the test set\n",
    "acc_test = get_accuracy(Y_test_hat, y_test.reshape((1, -1)))\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlQOsIj5GRXQ"
   },
   "source": [
    "### Question 4\n",
    " a) What accuracy did you get?\n",
    "\n",
    " b) Is this good?\n",
    "\n",
    "\n",
    "### Question 5\n",
    "To get better pefromance you can increase the capacity of the network. This can be done by increasing its size.\n",
    " * Go back to the cell where the network architecture was defined (i.e. the second cell, right below **Figure 1**).\n",
    " * Change the number of neurons in the first hidden layer from 4 to 25, in the second hidden layer from 6 to 50, in the third hidden layer from 6 to 50 and in the final fourth hidden layer change the number of neurons from 4 to 25. The new architecture should look like this:\n",
    "```\n",
    "  nn_architecture = [\n",
    "      {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "      {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "      {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "      {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "      {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "  ]\n",
    "```\n",
    " * Run all the cells again.\n",
    "\n",
    " a) What accuracy do you get with a larger capacity network? \n",
    "\n",
    " b) Is this good?\n",
    "\n",
    "## Visualisation of the learning process\n",
    "\n",
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zpm8IfwcGRXS"
   },
   "outputs": [],
   "source": [
    "# boundary of the graph\n",
    "GRID_X_START = -1.5\n",
    "GRID_X_END = 2.5\n",
    "GRID_Y_START = -1.0\n",
    "GRID_Y_END = 2\n",
    "# output directory (the folder must be created in the current working directory)\n",
    "OUTPUT_DIR = \"./fig/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_P8WwMrSE1z"
   },
   "outputs": [],
   "source": [
    "# test is OUTPUT_DIR was created\n",
    "import os\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "  print('Make the directory fig in the current working directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTo0yeLPGRXj"
   },
   "source": [
    "### Grid boundaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fU9ag8cGRXn"
   },
   "outputs": [],
   "source": [
    "grid = np.mgrid[GRID_X_START:GRID_X_END:100j,GRID_Y_START:GRID_Y_END:100j]\n",
    "grid_2d = grid.reshape(2, -1).T\n",
    "XX, YY = grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R__rNtUhGRXt"
   },
   "source": [
    "### Callback function for plotting changes during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGaY6wHGGRXu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def callback(i, weights, error):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    plot_title = \"Model - epoch: %05d - error: %1.4f\" % (i, error)\n",
    "    fname = \"model_{:05}.png\".format(i//50)\n",
    "    fpath = os.path.join(OUTPUT_DIR, fname)\n",
    "    prediction_probs, _ = full_fwdprop(grid_2d.T, weights, nn_architecture)\n",
    "    prediction_probs = prediction_probs.reshape(prediction_probs.shape[1], 1)\n",
    "    make_plot(X_test, y_test, plot_title, fname=fpath, XX=XX, YY=YY, preds=prediction_probs, dark=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnxJ0Qs1GRX1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "n_epochs = 10000\n",
    "learning_rate = 0.01\n",
    "# weights = train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), NN_ARCHITECTURE, 10000, 0.01, False, callback_numpy_plot)\n",
    "weights = train(X_train.T, y_train.reshape((1, -1)), nn_architecture, n_epochs, learning_rate, verbose=False, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTML40Gn7SnI"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "Y_test_hat, _ = full_fwdprop(X_test.T, weights, nn_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqbmX_4XGRX7"
   },
   "outputs": [],
   "source": [
    "# Accuracy achieved on the test set\n",
    "acc_test = get_accuracy(Y_test_hat, y_test.reshape((1, -1)))\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BFUvmz17eLI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MBAN6500_class04_NN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
